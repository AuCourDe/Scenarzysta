# üöÄ INSTRUKCJA URUCHOMIENIA - SCENARZYSTA

## üìã Spis tre≈õci
1. [Szybki start](#szybki-start)
2. [Wymagania](#wymagania)
3. [Instalacja](#instalacja)
4. [Uruchomienie](#uruchomienie)
5. [RozwiƒÖzywanie problem√≥w](#rozwiƒÖzywanie-problem√≥w)

---

## ‚ö° Szybki start

### **Linux / WSL / macOS:**
```bash
cd /≈õcie≈ºka/do/Scenarzysta
chmod +x run.sh
./run.sh
```

### **Windows:**
```cmd
cd C:\≈õcie≈ºka\do\Scenarzysta
run.bat
```

**To wszystko!** Skrypt automatycznie:
- ‚úÖ Sprawdzi wymagania systemowe
- ‚úÖ Sprawdzi czy Ollama dzia≈Ça
- ‚úÖ Sprawdzi czy model jest pobrany
- ‚úÖ Zainstaluje zale≈ºno≈õci Python
- ‚úÖ Uruchomi aplikacjƒô

Interfejs web bƒôdzie dostƒôpny pod: **http://localhost:5000**

---

## üì¶ Wymagania

### **1. System operacyjny:**
- Linux (Ubuntu 20.04+, Debian 11+, inne dystrybucje)
- Windows 10/11 (64-bit)
- macOS 11+ (Big Sur lub nowszy)
- WSL2 na Windows

### **2. Python:**
- **Wersja**: Python 3.8 lub nowsza
- **Sprawd≈∫**: `python3 --version` lub `python --version`

**Instalacja Python:**
- **Ubuntu/Debian**: `sudo apt install python3 python3-pip`
- **Windows**: [python.org/downloads](https://www.python.org/downloads/)
- **macOS**: `brew install python3`

### **3. Ollama:**
- **Wersja**: Najnowsza
- **Wymagana**: Do analizy dokument√≥w i generowania scenariuszy

**Instalacja Ollama:**

**Linux / WSL / macOS:**
```bash
curl -fsSL https://ollama.com/install.sh | sh
```

**Windows:**
- Pobierz z: [ollama.com/download](https://ollama.com/download)
- Zainstaluj i uruchom

### **4. Model AI:**
- **Wymagany**: `gemma3:12b` (7.4GB) **LUB** `gemma2:2b` (1.6GB)
- **Rekomendowany**: `gemma3:12b` (lepsza jako≈õƒá)

**Pobierz model:**
```bash
ollama pull gemma3:12b
# LUB mniejszy:
ollama pull gemma2:2b
```

### **5. GPU (opcjonalne, ale zalecane):**
- **NVIDIA GPU**: CUDA-compatible
- **Minimalna pamiƒôƒá**: 8GB VRAM
- **Rekomendowana**: 16GB+ VRAM (dla gemma3:12b)
- **Wspierane**: T4, RTX 3060+, A100, etc.

Bez GPU model dzia≈Ça na CPU (wolniej, ale dzia≈Ça).

---

## üîß Instalacja

### **Krok 1: Pobierz projekt**

```bash
cd ~/projects
# Je≈õli masz git:
git clone https://github.com/twoj-repo/Scenarzysta.git
cd Scenarzysta

# LUB rozpakuj archiwum ZIP i przejd≈∫ do folderu
```

### **Krok 2: Zainstaluj Ollama (je≈õli nie masz)**

**Linux / WSL / macOS:**
```bash
curl -fsSL https://ollama.com/install.sh | sh
```

**Windows:**
1. Pobierz instalator z [ollama.com/download](https://ollama.com/download)
2. Uruchom instalator
3. Otw√≥rz Ollama (powinna dzia≈Çaƒá w tle)

### **Krok 3: Uruchom Ollama**

**Linux / WSL / macOS:**
```bash
ollama serve
# Zostaw ten terminal otwarty, otw√≥rz nowy terminal dla dalszych krok√≥w
```

**Windows:**
- Ollama powinna siƒô uruchomiƒá automatycznie po instalacji
- Je≈õli nie, uruchom "Ollama" z menu Start

### **Krok 4: Pobierz model**

```bash
# W nowym terminalu:
ollama pull gemma3:12b

# Postƒôp pobierania bƒôdzie wy≈õwietlany
# Poczekaj a≈º pobierze siƒô ca≈Çy model (~7.4GB)
```

**Sprawd≈∫ czy model jest dostƒôpny:**
```bash
ollama list
# Powinien pojawiƒá siƒô: gemma3:12b
```

### **Krok 5: Zainstaluj zale≈ºno≈õci Python**

**Opcja A: Z wirtualnym ≈õrodowiskiem (zalecane):**
```bash
cd Scenarzysta

# Utw√≥rz venv
python3 -m venv venv

# Aktywuj venv
# Linux/macOS/WSL:
source venv/bin/activate
# Windows:
venv\Scripts\activate

# Zainstaluj zale≈ºno≈õci
pip install -r requirements.txt
```

**Opcja B: Bez venv (globalnie):**
```bash
cd Scenarzysta
pip3 install -r requirements.txt
# lub: pip install -r requirements.txt
```

---

## üé¨ Uruchomienie

### **Metoda 1: Automatyczny skrypt (ZALECANE)**

**Linux / WSL / macOS:**
```bash
chmod +x run.sh
./run.sh
```

**Windows:**
```cmd
run.bat
```

Skrypt przeprowadzi Ciƒô przez ca≈Çy proces i uruchomi aplikacjƒô.

---

### **Metoda 2: Rƒôczne uruchomienie**

**Krok 1: Upewnij siƒô, ≈ºe Ollama dzia≈Ça**
```bash
# Test:
curl http://localhost:11434/api/version
# Powinno zwr√≥ciƒá wersjƒô Ollama
```

**Krok 2: Aktywuj venv (je≈õli u≈ºywasz)**
```bash
# Linux/macOS/WSL:
source venv/bin/activate
# Windows:
venv\Scripts\activate
```

**Krok 3: Uruchom aplikacjƒô**
```bash
python3 main.py
# lub: python main.py
```

**Krok 4: Otw√≥rz przeglƒÖdarkƒô**
```
http://localhost:5000
```

---

## üåê U≈ºywanie aplikacji

### **1. Otw√≥rz interfejs web:**
```
http://localhost:5000
```

### **2. Prze≈õlij dokument:**
- Kliknij "Wybierz plik" lub przeciƒÖgnij plik
- **Format**: `.docx` (Microsoft Word)
- **Rozmiar**: Do 50 MB
- **Strony**: 1-800 stron ‚úÖ

### **3. Poczekaj na przetworzenie:**

**Szacowany czas:**
- **50 stron**: ~2-5 minut
- **200 stron**: ~8-12 minut
- **500 stron**: ~20-30 minut
- **800 stron**: ~35-50 minut

**Postƒôp jest wy≈õwietlany:**
- Procent uko≈Ñczenia
- Szacowany czas pozosta≈Çy
- Status: "Ekstrakcja", "Etap 1", "Etap 2", "Etap 3"

### **4. Pobierz wyniki:**
- **Format**: Excel (`.xlsx`)
- **Zawarto≈õƒá**:
  - ID scenariusza
  - Nazwa scenariusza
  - Numer kroku
  - Akcja
  - Oczekiwany rezultat
  - ≈πr√≥d≈Ço (sekcje dokumentacji)
  - Priorytet
  - Status

---

## üêõ RozwiƒÖzywanie problem√≥w

### **Problem 1: "Python nie jest zainstalowany"**

**RozwiƒÖzanie:**
```bash
# Ubuntu/Debian:
sudo apt update
sudo apt install python3 python3-pip

# macOS:
brew install python3

# Windows:
# Pobierz z python.org i zainstaluj
```

---

### **Problem 2: "Ollama nie dzia≈Ça"**

**Sprawd≈∫ czy dzia≈Ça:**
```bash
curl http://localhost:11434/api/version
```

**Je≈õli nie dzia≈Ça:**

**Linux/macOS/WSL:**
```bash
# Uruchom w osobnym terminalu:
ollama serve
```

**Windows:**
```
1. Otw√≥rz "Ollama" z menu Start
2. LUB uruchom w CMD: ollama serve
```

**Je≈õli nadal nie dzia≈Ça:**
```bash
# Sprawd≈∫ czy port 11434 jest zajƒôty:
# Linux/macOS:
lsof -i :11434
# Windows:
netstat -ano | findstr :11434

# Je≈õli zajƒôty przez inny proces, zabij go lub zmie≈Ñ port w app.py
```

---

### **Problem 3: "Model nie jest pobrany"**

**Pobierz model:**
```bash
ollama pull gemma3:12b
```

**Sprawd≈∫ dostƒôpne modele:**
```bash
ollama list
```

**Je≈õli brak miejsca:**
```bash
# U≈ºyj mniejszego modelu:
ollama pull gemma2:2b

# Zmie≈Ñ w app.py:
# ollama_model = "gemma2:2b"
```

---

### **Problem 4: "B≈ÇƒÖd podczas instalacji zale≈ºno≈õci"**

**RozwiƒÖzanie 1: Upgrade pip**
```bash
pip3 install --upgrade pip
pip3 install -r requirements.txt
```

**RozwiƒÖzanie 2: Zainstaluj pojedynczo**
```bash
pip3 install Flask flask-cors
pip3 install python-docx openpyxl Pillow
pip3 install requests
```

**RozwiƒÖzanie 3: U≈ºyj conda**
```bash
conda create -n scenarzysta python=3.10
conda activate scenarzysta
pip install -r requirements.txt
```

---

### **Problem 5: "Aplikacja siƒô zawiesza przy d≈Çugim dokumencie"**

**Przyczyna**: Timeout lub brak pamiƒôci

**RozwiƒÖzanie 1: Zwiƒôksz timeout**
```python
# W document_processor.py, funkcja _call_ollama():
response = requests.post(api_url, json=payload, timeout=300)  # By≈Ço 120
```

**RozwiƒÖzanie 2: Zmniejsz chunk size**
```python
# W document_processor.py:
doc_chunks = self._split_documentation_into_chunks(doc_text, max_tokens=8000)  # By≈Ço 12000
```

**RozwiƒÖzanie 3: U≈ºyj mniejszego modelu**
```bash
ollama pull gemma2:2b
# Zmie≈Ñ w app.py: ollama_model = "gemma2:2b"
```

---

### **Problem 6: "Ollama zwraca puste odpowiedzi"**

**Przyczyna**: Model wyczerpa≈Ç pamiƒôƒá lub przekroczono limit kontekstu

**RozwiƒÖzanie:**
```bash
# Restart Ollama:
# Linux/macOS:
pkill ollama
ollama serve

# Windows:
# Zamknij Ollama i uruchom ponownie
```

**Je≈õli nadal nie dzia≈Ça:**
```
1. Zmniejsz max_tokens w settings.txt do 4096
2. Zmniejsz chunk size do 8000 token√≥w
3. U≈ºyj modelu z wiƒôkszym kontekstem (je≈õli masz pamiƒôƒá)
```

---

### **Problem 7: "Brak pamiƒôci GPU"**

**Objaw**: "CUDA out of memory" lub wolne przetwarzanie

**RozwiƒÖzanie 1: U≈ºyj mniejszego modelu**
```bash
ollama pull gemma2:2b  # 1.6GB zamiast 7.4GB
```

**RozwiƒÖzanie 2: Wymuszenie CPU**
```bash
# Przed uruchomieniem:
export CUDA_VISIBLE_DEVICES=""
python3 main.py
```

**RozwiƒÖzanie 3: Zmniejsz batch size**
```
W settings.txt zmie≈Ñ max_tokens=4096
```

---

## üìä Parametry wydajno≈õci

### **Dla GPU T4 (16GB VRAM):**
```
Model: gemma3:12b
max_tokens: 8192
Chunk size: 12000 token√≥w
Dokumenty: Do 800 stron ‚úÖ
```

### **Dla GPU z 8GB VRAM:**
```
Model: gemma2:2b
max_tokens: 4096
Chunk size: 8000 token√≥w
Dokumenty: Do 500 stron ‚úÖ
```

### **Dla CPU (bez GPU):**
```
Model: gemma2:2b (zalecany)
max_tokens: 2048-4096
Czas: ~10x wolniejszy
Dokumenty: Do 200 stron (zalecane)
```

---

## üéØ Wskaz√≥wki optymalizacji

### **1. Dla d≈Çugich dokument√≥w (500+ stron):**
- ‚úÖ Fragmentacja jest W≈ÅƒÑCZONA automatycznie
- ‚úÖ Chunk size: 12000 token√≥w (mo≈ºna zmniejszyƒá do 8000-10000)
- ‚úÖ max_tokens: 8192 (mo≈ºna zmniejszyƒá do 6144-4096)

### **2. Dla szybszego przetwarzania:**
- U≈ºyj GPU zamiast CPU
- Zwiƒôksz `num_ctx` w Ollama: `ollama run gemma3:12b --num_ctx 16384`
- U≈ºyj SSD zamiast HDD

### **3. Dla lepszej jako≈õci:**
- U≈ºyj `gemma3:12b` zamiast `gemma2:2b`
- Zwiƒôksz `max_tokens` do 8192+
- Zwiƒôksz `temperature` do 0.3-0.4 dla bardziej kreatywnych odpowiedzi

---

## üìû Pomoc i wsparcie

### **Dokumentacja:**
- `README.md` - Og√≥lne info o projekcie
- `ANALIZA_I_REKOMENDACJE.md` - Analiza i usprawnienia
- `FRAGMENTACJA_DLA_DLUGICH_DOKUMENTOW.md` - Fragmentacja
- `PRZYKLADY_IMPLEMENTACJI.md` - Przyk≈Çady kodu

### **Logi aplikacji:**
Sprawd≈∫ terminal/konsolƒô gdzie uruchomi≈Çe≈õ aplikacjƒô - tam sƒÖ wszystkie logi.

### **Problemy z Ollama:**
```bash
# Sprawd≈∫ logi Ollama:
journalctl -u ollama  # Linux
# Lub sprawd≈∫: ~/.ollama/logs/
```

---

## ‚úÖ Checklist startowa

Przed pierwszym uruchomieniem upewnij siƒô:

- [ ] Python 3.8+ zainstalowany
- [ ] pip zainstalowany
- [ ] Ollama zainstalowana
- [ ] Ollama dzia≈Ça (`curl http://localhost:11434/api/version`)
- [ ] Model pobrany (`ollama list | grep gemma3`)
- [ ] Zale≈ºno≈õci Python zainstalowane (`pip list | grep Flask`)
- [ ] Foldery utworzone (`user_data/`, `trash/`)
- [ ] Pliki konfiguracyjne obecne (`prompt1.txt`, `prompt2.txt`, `prompt3.txt`, `settings.txt`)

**Je≈õli wszystko OK** ‚Üí Uruchom `./run.sh` lub `run.bat`!

---

**üéâ Gotowe! Powodzenia z testowaniem!** üöÄ
