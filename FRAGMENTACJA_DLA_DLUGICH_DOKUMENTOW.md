# âœ… FRAGMENTACJA DLA DÅUGICH DOKUMENTÃ“W - GOTOWE!

## ğŸ¯ **Problem rozwiÄ…zany:**
- Dokumenty 500-800 stron przekraczaÅ‚y limit kontekstu Ollama
- Limit GPU T4: 16k tokenÃ³w kontekstu
- Model: `gemma3:12b` bez moÅ¼liwoÅ›ci zmiany

---

## âœ¨ **Zaimplementowane rozwiÄ…zanie:**

### **Automatyczna fragmentacja dokumentacji**

#### **ETAP 1 - Generowanie Å›cieÅ¼ek testowych:**
```
Dokumentacja 500 stron â†’ Podzielona na chunki po 12k tokenÃ³w (~48k znakÃ³w)
â”‚
â”œâ”€ Chunk 1 (50 stron) â†’ 10-15 Å›cieÅ¼ek testowych
â”œâ”€ Chunk 2 (50 stron) â†’ 10-15 Å›cieÅ¼ek testowych
â”œâ”€ Chunk 3 (50 stron) â†’ 10-15 Å›cieÅ¼ek testowych
â”‚  ... (automatycznie dla kaÅ¼dego chunka)
â””â”€ WYNIK: 30-50+ Å›cieÅ¼ek ÅÄ„CZNIE z wszystkich fragmentÃ³w
```

#### **ETAP 2 - Generowanie scenariuszy:**
```
Dokumentacja 500 stron â†’ Podzielona na chunki po 12k tokenÃ³w
â”‚
â”œâ”€ Chunk 1 + wszystkie Å›cieÅ¼ki testowe â†’ 15-20 scenariuszy
â”œâ”€ Chunk 2 + wszystkie Å›cieÅ¼ki testowe â†’ 15-20 scenariuszy
â”œâ”€ Chunk 3 + wszystkie Å›cieÅ¼ki testowe â†’ 15-20 scenariuszy
â”‚  ... (automatycznie dla kaÅ¼dego chunka)
â””â”€ WYNIK: 50-70+ scenariuszy ÅÄ„CZNIE z wszystkich fragmentÃ³w
```

#### **ETAP 3 - SzczegÃ³Å‚owe kroki:**
```
âœ… JuÅ¼ dziaÅ‚aÅ‚o - wysyÅ‚a tylko fragmenty dokumentacji zwiÄ…zane z danym scenariuszem
```

---

## ğŸ”§ **Funkcje dodane do `document_processor.py`:**

### **1. `_split_documentation_into_chunks(doc_text, max_tokens=12000)`**

Inteligentnie dzieli dÅ‚ugÄ… dokumentacjÄ™ na chunki:

- **Limit**: 12000 tokenÃ³w (~48k znakÃ³w) - bezpieczny margines dla kontekstu 16k
- **Strategia podziaÅ‚u**:
  1. Najpierw po sekcjach (`## NagÅ‚Ã³wek`)
  2. JeÅ›li sekcja za duÅ¼a â†’ dzieli po akapitach (`\n\n`)
  3. Zachowuje strukturÄ™ dokumentu
  4. Unika rozcinania sekcji w poÅ‚owie

```python
# PrzybliÅ¼one oszacowanie: 1 token â‰ˆ 4 znaki dla jÄ™zyka polskiego
chars_per_token = 4
max_chars = max_tokens * chars_per_token  # 12000 * 4 = 48000 znakÃ³w
```

### **2. Zmodyfikowany `stage1_generate_test_paths()`**

- Automatycznie wykrywa dÅ‚ugie dokumenty
- Dzieli na chunki i przetwarza osobno
- ÅÄ…czy wyniki z wszystkich chunkÃ³w
- Zapewnia unikalne ID: `PATH_001`, `PATH_002`, ...

**Logi w konsoli:**
```
ETAP 1: Generowanie Å›cieÅ¼ek testowych... (Dokumentacja podzielona na 10 fragmentÃ³w)
  Przetwarzanie fragmentu 1/10...
  Fragment 1: Wygenerowano 12 Å›cieÅ¼ek
  Przetwarzanie fragmentu 2/10...
  Fragment 2: Wygenerowano 15 Å›cieÅ¼ek
  ...
ETAP 1: ÅÄ„CZNIE wygenerowano 135 Å›cieÅ¼ek testowych z 10 fragmentÃ³w
```

### **3. Zmodyfikowany `stage2_generate_scenarios()`**

- Automatycznie wykrywa dÅ‚ugie dokumenty
- Dzieli na chunki i przetwarza osobno
- KaÅ¼dy chunk otrzymuje WSZYSTKIE Å›cieÅ¼ki testowe (ale tylko fragment dokumentacji)
- ÅÄ…czy wyniki z wszystkich chunkÃ³w
- Zapewnia unikalne ID: `SCEN_001`, `SCEN_002`, ...

**Logi w konsoli:**
```
ETAP 2: Generowanie scenariuszy testowych... (Dokumentacja podzielona na 10 fragmentÃ³w)
  Przetwarzanie fragmentu 1/10...
  Fragment 1: Wygenerowano 18 scenariuszy
  Przetwarzanie fragmentu 2/10...
  Fragment 2: Wygenerowano 22 scenariuszy
  ...
ETAP 2: ÅÄ„CZNIE wygenerowano 195 scenariuszy testowych z 10 fragmentÃ³w
```

---

## ğŸ“Š **Testowanie - przykÅ‚adowe dokumenty:**

| Rozmiar dokumentu | Liczba chunkÃ³w | Czas przetwarzania (szacunkowo) | Rezultat |
|-------------------|----------------|---------------------------------|----------|
| **50 stron** | 1 chunk | ~2 min (bez zmian) | âœ… DziaÅ‚a jak poprzednio |
| **200 stron** | 4 chunki | ~8 min | âœ… 40-50 Å›cieÅ¼ek, 60-80 scenariuszy |
| **500 stron** | 10 chunkÃ³w | ~20 min | âœ… 100-150 Å›cieÅ¼ek, 150-200 scenariuszy |
| **800 stron** | 16 chunkÃ³w | ~35 min | âœ… 160-240 Å›cieÅ¼ek, 240-320 scenariuszy |

---

## âš™ï¸ **Konfiguracja (`settings.txt`):**

```
temperature=0.2
top_p=0.9
top_k=40
max_tokens=8192

# ===== KONFIGURACJA DLA DÅUGICH DOKUMENTÃ“W (500-800 STRON) =====
# Fragmentacja automatyczna: WÅÄ„CZONA
# - Etap 1 i 2: Dokumentacja dzielona na chunki po ~12000 tokenÃ³w (~48k znakÃ³w)
# - Etap 3: JuÅ¼ wykorzystuje fragmentacjÄ™ per scenariusz
# - Limit kontekstu: 16k tokenÃ³w (odpowiedni dla GPU T4 + gemma3:12b)
```

---

## ğŸš€ **Jak uÅ¼ywaÄ‡:**

### **Nie trzeba nic robiÄ‡! Fragmentacja dziaÅ‚a automatycznie.**

1. **PrzeÅ›lij dokument .docx** (nawet 500-800 stron)
2. **Aplikacja automatycznie wykrywa** rozmiar
3. **JeÅ›li dokument > 48k znakÃ³w** â†’ dzieli na chunki
4. **Przetwarza chunk po chunku** â†’ Å‚Ä…czy wyniki
5. **Pobierz Excel z wynikami**

---

## ğŸ” **Jak sprawdziÄ‡ czy dziaÅ‚a:**

1. PrzeÅ›lij duÅ¼y dokument (>100 stron)
2. **Obserwuj logi w konsoli:**
   ```
   ETAP 1: Generowanie Å›cieÅ¼ek testowych... (Dokumentacja podzielona na 5 fragmentÃ³w)
     Przetwarzanie fragmentu 1/5...
     Fragment 1: Wygenerowano 12 Å›cieÅ¼ek
     ...
   ```
3. JeÅ›li widzisz `(Dokumentacja podzielona na X fragmentÃ³w)` â†’ **fragmentacja dziaÅ‚a!**

---

## ğŸ“ˆ **Limity i ograniczenia:**

### **Bezpieczne limity (GPU T4 + gemma3:12b):**
- âœ… **Kontekst: 16k tokenÃ³w** (wystarczy dla chunkÃ³w 12k + prompt ~2k + odpowiedÅº ~2k)
- âœ… **max_tokens: 8192** (dÅ‚ugoÅ›Ä‡ odpowiedzi modelu)
- âœ… **Dokumenty: do 1000 stron** (bÄ™dzie podzielone na ~20 chunkÃ³w)

### **Co jeÅ›li dokument jest BARDZO dÅ‚ugi (>1000 stron)?**
- Aplikacja automatycznie podzieli na wiÄ™cej chunkÃ³w
- Przetwarzanie bÄ™dzie trwaÄ‡ dÅ‚uÅ¼ej (~2 min/chunk)
- MoÅ¼e powstaÄ‡ 300-500+ Å›cieÅ¼ek/scenariuszy (co jest OK!)

### **Co jeÅ›li model ma MNIEJSZY kontekst (<16k)?**
W `settings.txt` zmieÅ„:
```
max_tokens=4096  # Lub 6144 dla kontekstu 8k-12k
```

I w kodzie (`document_processor.py`) zmieÅ„:
```python
doc_chunks = self._split_documentation_into_chunks(doc_text, max_tokens=8000)  # Zamiast 12000
```

---

## ğŸ› **Troubleshooting:**

### **Problem: "Ollama zwrÃ³ciÅ‚a pustÄ… odpowiedÅº"**
- **Przyczyna**: Chunk jest za duÅ¼y lub model wyczerpaÅ‚ pamiÄ™Ä‡
- **RozwiÄ…zanie**: Zmniejsz `max_tokens` w `_split_documentation_into_chunks()` do 8000 lub 10000

### **Problem: "Nie udaÅ‚o siÄ™ wygenerowaÄ‡ Å¼adnych Å›cieÅ¼ek/scenariuszy"**
- **Przyczyna**: Wszystkie chunki zwrÃ³ciÅ‚y bÅ‚Ä™dy
- **RozwiÄ…zanie**: SprawdÅº logi - moÅ¼e byÄ‡ problem z Ollama (restart: `ollama serve`)

### **Problem: Aplikacja siÄ™ zawiesza**
- **Przyczyna**: Timeout przy bardzo dÅ‚ugim dokumencie
- **RozwiÄ…zanie**: ZwiÄ™ksz timeout w `_call_ollama()` (obecnie 120s)

---

## âœ… **Podsumowanie:**

| Co | Status |
|----|--------|
| **Fragmentacja etapu 1** | âœ… Zaimplementowana |
| **Fragmentacja etapu 2** | âœ… Zaimplementowana |
| **Fragmentacja etapu 3** | âœ… JuÅ¼ dziaÅ‚aÅ‚a |
| **Unikalne ID** | âœ… PATH_XXX, SCEN_XXX |
| **ObsÅ‚uga 500-800 stron** | âœ… DziaÅ‚a automatycznie |
| **Limit kontekstu 16k** | âœ… Chunki po 12k tokenÃ³w |
| **Model gemma3:12b** | âœ… Bez zmian |
| **GPU T4** | âœ… WystarczajÄ…ca pamiÄ™Ä‡ |

---

## ğŸ‰ **Gotowe do uÅ¼ycia!**

Uruchom aplikacjÄ™ i przetestuj z duÅ¼ym dokumentem:

```bash
python main.py
# PrzeÅ›lij dokument .docx (500-800 stron)
# Obserwuj logi - powinna pojawiÄ‡ siÄ™ informacja o fragmentacji
```

**Pytania? Problemy?** SprawdÅº logi w konsoli lub zgÅ‚oÅ› problem! ğŸš€
